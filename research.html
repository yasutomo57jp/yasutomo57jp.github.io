<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-9b2LQHRPPE"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-9b2LQHRPPE');
    </script>
    <title>Yasutomo KAWANISHI - research</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="https://yasutomo57jp.github.io/assets/css/main.css" />
  </head>
  <body class="is-preload">
    <div id="page-wrapper">

      <!-- Header -->
      <div id="header">

        <!-- Logo -->
        <h1><a href="index.html" id="logo">川西康友</a></h1>

        <!-- Nav -->
        <nav id="nav">
          <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="profile.html">プロフィール</a></li>
            <li class="current"><a href="research.html">研究</a></li>
            <li><a href="publications.html">業績</a></li>
            <li><a href="datasets.html">データセット</a></li>
            <li>&nbsp;</li>
            <li>&nbsp;</li>
            <li><a href="#">[Language]</a>
              <ul>
                <li><a href="https://yasutomo57jp.github.io/research.html">Japanese &#x1f1ef;&#x1f1f5;</a></li>
                <li><a href="https://yasutomo57jp.github.io/en/research.html">English &#x1f1fa;&#x1f1f8;</a></li>
              </ul>
            </li>
          </ul>
        </nav>

      </div>

      <!-- Main -->
      <section class="wrapper style1">
        <div class="container">
          <div id="content">

            <!-- Content -->

            <div class="box post">
              <a href="#openworld" class="image left"><img src="https://yasutomo57jp.github.io/images/openset-sgg.png" alt="" /></a>
              <div class="inner">
                <h3>未知物体・未知事象の認識と記述</h3>
                <h4>概要</h4>
                <p>人間は，知らない物体を見たとき，それが何かわからなくても何らかの物体であると認識できますが，ロボットは物体検出器が学習した物体しか検出できません．そこで，未学習の物体を未学習であると認識するOpen-set認識，及び新しい語彙で指定された対象を認識するOpen-vocabulary認識のタスクに取り組んでいます．また，物体だけでなく，行動など様々な未知事象に関する認識研究に対象を広げています．（この研究は日本学術振興会 科研費 基盤B 「拡張時空間シーングラフによる未知物体を含むシーン認識・記述基盤の構築」，科研費 基盤A 「能動的Open-world認識による知識拡張・環境認識基盤の構築」の支援を受けています．）</p>
                <h4>代表的な研究</h4>
                <ul>
                  <li>M. Sonogashira et al., Relationship-Aware Unknown Object Detection for Open-Set Scene Graph Generation, IEEE Access, 2024.</li>
                  <li>T. T. Nguyen et al., One-stage open-vocabulary temporal action detection leveraging temporal multi-scale and action label features, FG2024.</li>
                  <li>T. T. Nguyen et al., Zero-Shot Pill-Prescription Matching With Graph Convolutional Network and Contrastive Learning, IEEE Access, 2024.</li>

                  <li>薗頭ら，Open-setシーングラフ生成のための物体間の関係を考慮した未知物体検出, MIRU2023.</li>
                  <li>M. Sonogashira &amp; Y. Kawanishi, Towards Open-Set Scene Graph Generation with Unknown Objects, IEEE Access, 2022.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#scenegraph" class="image left"><img src="https://yasutomo57jp.github.io/images/openset-sgg.png" alt="" /></a>
              <div class="inner">
                <h3>画像からのシーングラフ生成とその応用</h3>
                <h4>概要</h4>
                <p>環境を高度に理解するため，物体の検出だけでなく，それらの物体同士の関係を推定・記述するシーングラフ生成というタスクに注目しています．また，それを応用し，複数の画像の内容を要約したり，キャプションを生成する研究に取り組んでいます．（この研究は日本学術振興会 科研費 基盤B 「拡張時空間シーングラフによる未知物体を含むシーン認識・記述基盤の構築」の支援を受けています．）</p>
                <h4>代表的な研究</h4>
                <ul>
                  <li>I. Phueaksri et al., Toward Visual Storytelling using Scene-Graph Contexts, MIRU2024, 2024.</li>
                  <li>I. Phueaksri et al., Image-Collection Summarization using Scene-Graph Generation with External Knowledge, IEEE Access, 2024.</li>
                  <li>I. Phueaksri et al., An Approach to Generate a Caption for an Image Collection using Scene Graph Generation, IEEE Access, 2023.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#multimodal" class="image left"><img src="https://yasutomo57jp.github.io/images/multimodal.jpg" alt="" /></a>
              <div class="inner">
                <h3>マルチモーダル情報を統合した認識基盤</h3>
                <h4>概要</h4>
                <p>人間は目や耳を使って，映像や音声のモダリティをもとに周囲の情報を得ています．複数モダリティをもとに物事を認識する，マルチモーダル認識に関する研究をしています．特に，複数のモダリティが全て揃っていない状況での認識（モダリティ欠損）の問題に取り組んでいます．</p>

                <h4>代表的な研究</h4>
                <ul>
                  <li>V. John and Y. Kawanishi, Progressive Learning of a Multimodal Classifier Accounting for Different Modality Combinationsi, Sensors, 2023.</li>
                  <li>V. John and Y. Kawanishi, A Multimodal Sensor Fusion Framework Robust to Missing Modalities for Person Recognition, arxiv:2210.10972, 2022.</li>
                  <li>V. John and Y. Kawanishi, Combining Knowledge Distillation and Transfer Learning for Sensor Fusion in Visible and Thermal Camera-based Person Classification, MVA2023.</li>
                  <li>V. John and Y. Kawanishi, Multimodal Cascaded Framework with Metric Learning Robust to Missing Modalities for Person Classification, ACM MMSys', 2023.</li>
                  <li>V. John and Y. Kawanishi, Audio-Visual Sensor Fusion Framework using Person Attributes Robust to Missing Visual Modality for Person Recognition, MMM2023.</li>
                  <li>V. John and Y. Kawanishi, A Multimodal Sensor Fusion Framework Robust to Missing Modalities for Person Recognition, ACM MM Asia 2022.</li>
                  <li>V. John and Y. Kawanishi, Audio and Video-Based Emotion Recognition Using Multimodal Transformers, ICPR2022.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#lfir" class="image left"><img src="https://yasutomo57jp.github.io/images/lfir.png" alt="" /></a>
              <div class="inner">
                <h3>超低解像度遠赤外線画像の認識</h3>
                <h4>概要</h4>
                <p>高齢者の見守りのためには，プライバシに考慮し，また夜間でも認識可能なセンシングが必要です．そこで，超低解像度な遠赤外線画像に着目しています．超低解像度であるためにプライバシは問題にならず，遠赤外線を捉えることで夜間でもセンシングが出来ます．これを用いて，人物の行動認識や姿勢推定に関する研究をしています．</p>

                <h4>代表的な研究</h4>
                <ul>
                  <li>S. Iwata et al., LFIR2Pose: Pose Estimation from an Extremely Low-Resolution FIR Image Sequence, ICPR2020.</li>
                  <li>Y. Kawanishi et al., Voting-based Hand-Waving Gesture Spotting from a Low-Resolution Far-Infrared Image Sequence, VCIP2018.</li>
                  <li>T. Kawashima et al., Action Recognition from Extremely Low-Resolution Thermal Image Sequence, AVSS2017.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#objpose" class="image left"><img src="https://yasutomo57jp.github.io/images/objpose.png" alt="" /></a>
              <div class="inner">
                <h3>距離画像を用いた物体姿勢推定</h3>
                <h4>概要</h4>
                <p>ロボットが物体を掴んだり操作したりする際には，物体の姿勢を正しく認識する必要があります．最近は距離画像センサが多くのロボットに搭載されるようになって来ています．そこで，距離画像を用いて物体の姿勢を推定する研究に取り組んでいます．</p>
                <h4>代表的な研究</h4>
                <ul>
                  <li>H. Tatemichi et al., Category-level Object Pose Estimation in Heavily Cluttered Scenes by Generalized Two-stage Shape Reconstructor, IEEE Access, 2024.</li>
                  <li>N. M. Z. Hashim et al., Best Next-Viewpoint Recommendation by Selecting Minimum Pose Ambiguity for Category-level Object Pose Estimation, JSPE, 2021.</li>
                  <li>H. Tatemichi et al., Median-Shape Representation Learning for Category-Level Object Pose Estimation in Cluttered Environments, ICPR2020.</li>
                  <li>H. Ninomiya et al., Deep Manifold Embedding for 3D Object Pose Estimation, VISAPP2017.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#gaze" class="image left"><img src="https://yasutomo57jp.github.io/images/gaze.png" alt="" /></a>
              <div class="inner">
                <h3>群衆の視線推定</h3>
                <h4>概要</h4>
                <p>多数の人物が，何に注目しているのかを知ることは，スポーツ観戦者や音楽ライブイベント参加者の興味の推定に有用です．そこで，多数の人物を同時に写した映像から，そこに写っている人物の多くが，何に注目しているのかを推定する研究に取り組んでいます．</p>
                <h4>代表的な研究</h4>
                <ul>
                  <li>武田ら, 観衆の顔向きの時空間統合による注目対象の位置及び被注目度の推定, 電子情報通信学会論文誌A, 2023.</li>
                  <li>Y. Kodama et al., Localizing the Gaze Target of a Crowd of People, ACCV2018 Workshop.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#posesequence" class="image left"><img src="https://yasutomo57jp.github.io/images/posesequence.png" alt="" /></a>
              <div class="inner">
                <h3>骨格系列の変化による行動認識・予測</h3>
                <h4>概要</h4>
                <p>人の骨格がどのように変化しているかに着目することで，その人が何をしているのか，どういう状態なのかを知ることが出来ます．最近，画像から人物の骨格を精度よく推定できる技術が出てきたことから，こうした技術を用いて推定した骨格を，様々な認識や予測に応用する研究に取り組んでいます．特に，見た目からだけでは判断しづらい，荷物の重さや，手荷物所持者がどのくらい負担に感じているか，等を認識することにも取り組んでいます．</p>
                <h4>代表的な研究</h4>
                <ul>
                  <li>M. Mizuno et al., Subjective Baggage-Weight Estimation based on Human Walking Behavior, IEEE Access, 2024.</li>
                  <li>M. Mizuno et al., Subjective Baggage-Weight Estimation from Gait ---Can you estimate how heavy the person feels?---, VISAPP2023.</li>
                  <li>T. Fujita, Future Pose Prediction from 3D Human Skeleton Sequence with Surrounding Situation, Sensors, 2023.</li>
                  <li>T. Fujita et al., Human Pose Prediction by Progressive Generation in Multi-scale Frequency Domain, MVA2023.</li>
                  <li>T. Fujita et al., Toward Surroundings-aware Temporal Prediction of 3D Human Skeleton Sequence, ICPR2022 Workshop (T-CAP).</li>
                  <li>藤田ら, 人物周辺情報を活用した3次元骨格の時系列予測, MIRU2022.</li>
                  <li>水野ら, 歩容からの身体特徴と動作特徴の分離による手荷物の物理的・主観的重さ推定, MIRU2022.</li>
                  <li>水野ら, 個人差を考慮した歩き方からの手荷物の重さ推定の検討, 電子情報通信学会技術研究報告(PRMU), PRMU2021-56, 2021/12/17</li>
                  <li>N. Nishida et al., SOANets: Encoder-Decoder based Skeleton Orientation Alignment Network for White Cane User Recognition from 2D Human Skeleton Sequence, VISAPP2020.</li>
                  <li>O. Temuroglu et al, Occlusion-Aware Skeleton Trajectory Representation for Abnormal Behavior Detection, IW-FCV2020.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#mtmct" class="image left"><img src="https://yasutomo57jp.github.io/images/mtmct.png" alt="" /></a>
              <div class="inner">
                <h3>広範囲の複数人物追跡</h3>
                <h4>概要</h4>
                <p>広範囲にわたる人物の移動軌跡を知ることは，人流解析や迷子探し等の技術において重要です．複数のカメラによって広範囲を観測する状況で，どの人がどこを通ってどこへ行ったかを知るために，各カメラ視野内での人物追跡と，カメラ視野間での人物の対応付けに関する研究に取り組んでいます．</p>
                <h4>代表的な研究</h4>
                <ul>
                  <li>Y. Kawanishi, Label-Based Multiple Object Ensemble Tracking with Randomized Frame Dropping, ICPR2022.</li>
                  <li>Y. Kawanishi et al., Trajectory Ensemble: Multiple Persons Consensus Tracking across Non-overlapping Multiple Cameras over Randomly Dropped Camera Networks, CVPR2017 Workshop.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#otherfields" class="image left"><img src="https://yasutomo57jp.github.io/images/otherfields.png" alt="" /></a>
              <div class="inner">
                <h3>画像認識技術の他分野への応用</h3>
                <h4>概要</h4>
                <p>画像認識技術は，生命科学や天文学など，様々な分野でも期待されています．我々も，天文学や考古学の分野に画像認識技術を応用した研究に取り組んでいます．例えば，星が形成されつつある領域の検出であったり，発掘した土器の産地同定などに取り組んでいます．</p>
                <h4>代表的な研究</h4>
                <ul>
                  <li>Y. Kawae, et al., 3D Survey of the Menkaure Pyramid, Virtual Annual Meeting, American Research Center in Egypt, 2024.</li>
                  <li>Y. Shimajiri et al., Predicting reliable H2 column density maps from molecular line data using machine learning, MNRAS, 2023.</li>
                  <li>S. Fujita et al., Distance determination of molecular clouds in the first quadrant of the Galactic plane using deep learning, Protostars and Planets VII, 2023.</li>
                  <li>S. Fujita et al., Distance determination of molecular clouds in the first quadrant of the Galactic plane using deep learning: I. Method and results, ASJ, 2023.</li>
                  <li>S. Nishimoto et al., Development of a high-speed identification model for infrared-ring structures using deep learningi, SPIE Astronomical Telescopes + Instrumentation 2022.</li>
                  <li>堀ら, 埋蔵文化財保護行政における3Dデータ活用 ―機械学習を活用した須恵器杯蓋の産地同定手法―, 情報文化学会誌, 2022.</li>
                  <li>野原ら, 3Dデータを活用した埋蔵文化財業務の効率化 ～実測図自動作成手法の提案～, 社会情報学会中部支部・研究発表論文集, 2021.</li>
                  <li>井上ら, 人工知能による機械学習を用いた須恵器資料の断面形状分析<br/>日本情報考古学会第43回講演論文集, 2020.</li>
                  <li>S. Ueda et al., Identification of infrared-ring structures by convolutional neural network, SPIE Astronomical Telescopes + Instrumentation 2020</li>
                </ul>
              </div>
            </div>

          </div>
        </div>
      </section>

      <section class="wrapper style1">
        <div class="container">
          <div id="content2">

            <!-- Content -->

            <div class="box post">
              <div class="inner">
                <h3>主な共同研究・連携先（終了したものを含む）</h3>
                <ul>
                  <li>京都大学</li>
                  <li>名古屋大学</li>
                  <li>九州工業大学</li>
                  <li>滋賀大学</li>
                  <li>国立天文台</li>
                  <li>核融合科学研究所</li>
                  <li>大阪府立大学</li>
                  <li>九州共立大学</li>
                  <li>株式会社ロココ</li>
                  <li>長崎大学</li>
                  <li>関西大学</li>
                  <li>愛知工科大学</li>
                  <li>近畿大学</li>
                  <li>国立台湾大学</li>
                  <li>国立精華大学</li>
                  <li>国立成功大学</li>
                  <li>東北大学</li>
                </ul>
              </div>
            </div>
          </div>
      </section>


      <!-- Footer -->
      <div id="footer">

        <!-- Icons -->
        <ul class="icons">
          <li><a href="https://twitter.com/yasutomo57jp" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
          <li><a href="https://www.facebook.com/yasutomo57jp" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
          <li><a href="https://github.com/yasutomo57jp" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
          <li><a href="#" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
          <li><a href="#" class="icon brands fa-google-plus-g"><span class="label">Google+</span></a></li>
        </ul>

        <!-- Copyright -->
        <div class="copyright">
          <ul class="menu">
            <li>&copy; Yasutomo KAWANISHI. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
          </ul>
        </div>

      </div>

    </div>

    <!-- Scripts -->
    <script src="https://yasutomo57jp.github.io/assets/js/jquery.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/jquery.dropotron.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/browser.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/breakpoints.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/util.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/main.js"></script>

  </body>
</html>
