<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-9b2LQHRPPE"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-9b2LQHRPPE');
    </script>
    <title>Yasutomo KAWANISHI - research (en)</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="https://yasutomo57jp.github.io/assets/css/main.css" />
  </head>
  <body class="is-preload">
    <div id="page-wrapper">

      <!-- Header -->
      <div id="header">

        <!-- Logo -->
        <h1><a href="index.html" id="logo">Yasutomo KAWANISHI</a></h1>

        <!-- Nav -->
        <nav id="nav">
          <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="profile.html">Profile</a></li>
            <li class="current"><a href="research.html">Research</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="datasets.html">Datasets</a></li>
            <li>&nbsp;</li>
            <li>&nbsp;</li>
            <li><a href="#">[Language]</a>
              <ul>
                <li><a href="https://yasutomo57jp.github.io/research.html">Japanese &#x1f1ef;&#x1f1f5;</a></li>
                <li><a href="https://yasutomo57jp.github.io/en/research.html">English &#x1f1fa;&#x1f1f8;</a></li>
              </ul>
            </li>
          </ul>
        </nav>

      </div>

      <!-- Main -->
      <section class="wrapper style1">
        <div class="container">
          <div id="content">

            <!-- Content -->

            <div class="box post">
              <a href="#opensetsgg" class="image left"><img src="https://yasutomo57jp.github.io/images/openset-sgg_en.png" alt="" /></a>
              <div class="inner">
                <h3>Open-set Scene Graph Generation</h3>
                <h4>Summary</h4>
                <p>When we humans see an unknown object, we can recognize it as some kind of object even if we don't know what it is, but robots can only detect objects that their object detectors have learned about and cannot estimate the relationship with other objects. We are researching the topic, Open-set Scene Graph Generation, which enables robot for detecting unknown objects and their relationship and describing them in a graph representation. (This work was supported in part by the MEXT (Ministry of Education, Culture, Sports, Science and Technology, JAPAN) through
Grant-in-Aid for Scientific Research under Grant JP21H03519.)</p>
                <h4>Publications</h4>
                <ul>
                  <li>M. Sonogashira & Y. Kawanishi, Towards Open-Set Scene Graph Generation with Unknown Objects, IEEE Access, 2022.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#multimodal" class="image left"><img src="https://yasutomo57jp.github.io/images/multimodal_en.jpg" alt="" /></a>
              <div class="inner">
                <h3>Multimodal Recognition Framework</h3>
                <h4>Summary</h4>
                <p>We humans use our eyes and ears to obtain information from surroundings. We are working on multimodal recognition framework, which recognizes things based on multiple modalities. In particular, we are working on the missing modality problem where some of the modalities are not available.</p>
                <h4>Publications</h4>
                <ul>
                  <li>V. John and Y. Kawanishi, Progressive Learning of a Multimodal Classifier Accounting for Different Modality Combinationsi, Sensors, 2023.</li>
                  <li>V. John and Y. Kawanishi, A Multimodal Sensor Fusion Framework Robust to Missing Modalities for Person Recognition, arxiv:2210.10972, 2022.</li>
                  <li>V. John and Y. Kawanishi, Combining Knowledge Distillation and Transfer Learning for Sensor Fusion in Visible and Thermal Camera-based Person Classification, MVA2023.</li>
                  <li>V. John and Y. Kawanishi, Multimodal Cascaded Framework with Metric Learning Robust to Missing Modalities for Person Classification, ACM MMSys', 2023.</li>
                  <li>V. John and Y. Kawanishi, Audio-Visual Sensor Fusion Framework using Person Attributes Robust to Missing Visual Modality for Person Recognition, MMM2023.</li>
                  <li>V. John and Y. Kawanishi, A Multimodal Sensor Fusion Framework Robust to Missing Modalities for Person Recognition, ACM MM Asia 2022.</li>
                  <li>V. John and Y. Kawanishi, Audio and Video-Based Emotion Recognition Using Multimodal Transformers, ICPR2022.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#lfir" class="image left"><img src="https://yasutomo57jp.github.io/images/lfir.png" alt="" /></a>
              <div class="inner">
                <h3>Human Recognition using a Low-resolution FIR Sensor</h3>
                <h4>Summary</h4>
                <p>To watch over the elderly, we need sensing that can be recognized even at night, with privacy in mind. What we are focusing on is extremely low-resolution far-infrared images. Since they are extremely low-resolution, privacy is not an issue, and since they capture far-infrared light, we can use them for nighttime sensing. We are researching human tracking, gesture recognition, behavior recognition, and pose estimation using these images.</p>
                <h4>Publications</h4>
                <ul>
                  <li>S. Iwata et al., LFIR2Pose: Pose Estimation from an Extremely Low-Resolution FIR Image Sequence, ICPR2020.</li>
                  <li>Y. Kawanishi et al., Voting-based Hand-Waving Gesture Spotting from a Low-Resolution Far-Infrared Image Sequence, VCIP2018.</li>
                  <li>T. Kawashima et al., Action Recognition from Extremely Low-Resolution Thermal Image Sequence, AVSS2017.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#objpose" class="image left"><img src="https://yasutomo57jp.github.io/images/objpose.png" alt="" /></a>
              <div class="inner">
                <h3>Object Pose Estimation from Depth Images</h3>
                <h4>Summary</h4>
                <p>When a robot grasps or manipulates an object, it needs to recognize the object's pose correctly. Recently, depth image sensors have been installed in many robots. Therefore, we are working on object pose estimation using depth images.</p>
                <h4>Publications</h4>
                <ul>
                  <li>H. Tatemichi et al., Category-level Object Pose Estimation in Heavily Cluttered Scenes by Generalized Two-stage Shape Reconstructor, IEEE Access, 2024.</li>
                  <li>N. M. Z. Hashim et al., Best Next-Viewpoint Recommendation by Selecting Minimum Pose Ambiguity for Category-level Object Pose Estimation, JSPE, 2021.</li>
                  <li>H. Tatemichi et al., Median-Shape Representation Learning for Category-Level Object Pose Estimation in Cluttered Environments, ICPR2020.</li>
                  <li>H. Ninomiya et al., Deep Manifold Embedding for 3D Object Pose Estimation, VISAPP2017.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#gaze" class="image left"><img src="https://yasutomo57jp.github.io/images/gaze.png" alt="" /></a>
              <div class="inner">
                <h3>Gaze Analysis of a Crowd of People</h3>
                <h4>Summary</h4>
                <p>Knowing what a large number of people are paying attention to is useful for estimating the interests of people watching sports or attending a live music event. Therefore, from a video observing many people simultaneously, we are working on a research project to estimate the target most people in a video are paying attention to.</p>
                <h4>Publications</h4>
                <ul>
                  <li>Y. Kodama et al., Localizing the Gaze Target of a Crowd of People, ACCV2018 Workshop.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#posesequence" class="image left"><img src="https://yasutomo57jp.github.io/images/posesequence.png" alt="" /></a>
              <div class="inner">
                <h3>Action Recognition from a Human Skeleton Sequence</h3>
                <h4>Summary</h4>
                <p>By focusing on how a person's skeleton changes, we can estimate what the person is doing and what state the person is in. Recently, technologies that can accurately estimate a person's skeleton from images have become available, and we are working on applying the estimated skeleton to various recognition and prediction tasks.</p>
                <h4>Publications</h4>
                <ul>
                  <li>M. Mizuno et al., Subjective Baggage-Weight Estimation based on Human Walking Behavior, IEEE Access, 2024.</li>
                  <li>M. Mizuno et al., Subjective Baggage-Weight Estimation from Gait ---Can you estimate how heavy the person feels?---, VISAPP2023.</li>
                  <li>T. Fujita, Future Pose Prediction from 3D Human Skeleton Sequence with Surrounding Situation, Sensors, 2023.</li>
                  <li>T. Fujita et al., Human Pose Prediction by Progressive Generation in Multi-scale Frequency Domain, MVA2023.</li>
                  <li>T. Fujita et al., Toward Surroundings-aware Temporal Prediction of 3D Human Skeleton Sequence, ICPR2022 Workshop (T-CAP).</li>
                  <li>N. Nishida et al., SOANets: Encoder-Decoder based Skeleton Orientation Alignment Network for White Cane User Recognition from 2D Human Skeleton Sequence, VISAPP2020.</li>
                  <li>O. Temuroglu et al, Occlusion-Aware Skeleton Trajectory Representation for Abnormal Behavior Detection, IW-FCV2020.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#mtmct" class="image left"><img src="https://yasutomo57jp.github.io/images/mtmct.png" alt="" /></a>
              <div class="inner">
                <h3>People Tracking</h3>
                <h4>Summary</h4>
                <p>In techniques such as people flow analysis and lost child search, it is important to know the trajectories of people walking around a large area. We are working on tracking and re-identifying people within/across camera views in order to know which person went where by passing which paths, in a situation where a large area is being observed by multiple cameras.</p>
                <h4>Publications</h4>
                <ul>
                  <li>Y. Kawanishi, Label-Based Multiple Object Ensemble Tracking with Randomized Frame Dropping, ICPR2022.</li>
                  <li>Y. Kawanishi et al., Trajectory Ensemble: Multiple Persons Consensus Tracking across Non-overlapping Multiple Cameras over Randomly Dropped Camera Networks, CVPR2017 Workshop.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#otherfields" class="image left"><img src="https://yasutomo57jp.github.io/images/otherfields.png" alt="" /></a>
              <div class="inner">
                <h3>Image Recognition Applications to Other Research Fields</h3>
                <h4>Summary</h4>
                <p>Image recognition techniques are expected to be used in various fields such as life science. We are also working on the application of image recognition technology in the fields of astronomy and archaeology. (Detection of star forming region in Astronomy field, Automatic origin identification of earthenware in Archaeology field.)</p>
                <h4>Publications</h4>
                <ul>
                  <li>Y. Kawae, et al., 3D Survey of the Menkaure Pyramid, Virtual Annual Meeting, American Research Center in Egypt, 2024.</li>
                  <li>Y. Shimajiri et al., Predicting reliable H2 column density maps from molecular line data using machine learning, MNRAS, 2023.</li>
                  <li>S. Fujita et al., Distance determination of molecular clouds in the first quadrant of the Galactic plane using deep learning, Protostars and Planets VII, 2023.</li>
                  <li>S. Fujita et al., Distance determination of molecular clouds in the first quadrant of the Galactic plane using deep learning: I. Method and results, ASJ, 2023.</li>
                  <li>S. Nishimoto et al., Development of a high-speed identification model for infrared-ring structures using deep learningi, SPIE Astronomical Telescopes + Instrumentation 2022.</li>
                  <li>S. Ueda et al., Identification of infrared-ring structures by convolutional neural network, SPIE Astronomical Telescopes + Instrumentation 2020</li>
                </ul>
              </div>
            </div>

          </div>
        </div>
      </section>

      <!-- Footer -->
      <div id="footer">

        <!-- Icons -->
        <ul class="icons">
          <li><a href="https://twitter.com/yasutomo57jp" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
          <li><a href="https://www.facebook.com/yasutomo57jp" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
          <li><a href="https://github.com/yasutomo57jp" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
          <li><a href="#" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
          <li><a href="#" class="icon brands fa-google-plus-g"><span class="label">Google+</span></a></li>
        </ul>

        <!-- Copyright -->
        <div class="copyright">
          <ul class="menu">
            <li>&copy; Yasutomo KAWANISHI. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
          </ul>
        </div>

      </div>

    </div>

    <!-- Scripts -->
    <script src="https://yasutomo57jp.github.io/assets/js/jquery.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/jquery.dropotron.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/browser.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/breakpoints.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/util.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/main.js"></script>

  </body>
</html>
