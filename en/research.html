<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-132984140-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-132984140-1');
    </script>
    <title>Yasutomo KAWANISHI</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="https://yasutomo57jp.github.io/assets/css/main.css" />
  </head>
  <body class="is-preload">
    <div id="page-wrapper">

      <!-- Header -->
      <div id="header">

        <!-- Logo -->
        <h1><a href="index.html" id="logo">Yasutomo KAWANISHI</a></h1>

        <!-- Nav -->
        <nav id="nav">
          <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="profile.html">Profile</a></li>
            <li class="current"><a href="research.html">Research</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="datasets.html">Datasets</a></li>
            <li>&nbsp;</li>
            <li>&nbsp;</li>
            <li><a href="#">[Language]</a>
              <ul>
                <li><a href="https://yasutomo57jp.github.io/research.html">Japanese &#x1f1ef;&#x1f1f5;</a></li>
                <li><a href="https://yasutomo57jp.github.io/en/research.html">English &#x1f1fa;&#x1f1f8;</a></li>
              </ul>
            </li>
          </ul>
        </nav>

      </div>

      <!-- Main -->
      <section class="wrapper style1">
        <div class="container">
          <div id="content">

            <!-- Content -->

            <div class="box post">
              <a href="#lfir" class="image left"><img src="https://yasutomo57jp.github.io/images/lfir.png" alt="" /></a>
              <div class="inner">
                <h3>Human Recognition using a Low-resolution FIR Sensor</h3>
                <h4>Summary</h4>
                <p>To watch over the elderly, we need sensing that can be recognized even at night, with privacy in mind. What we are focusing on is extremely low-resolution far-infrared images. Since they are extremely low-resolution, privacy is not an issue, and since they capture far-infrared light, we can use them for nighttime sensing. We are researching human tracking, gesture recognition, behavior recognition, and pose estimation using these images.</p>
                <h4>Publications</h4>
                <ul>
                  <li>S. Iwata et al., LFIR2Pose: Pose Estimation from an Extremely Low-Resolution FIR Image Sequence, ICPR2020.</li>
                  <li>Y. Kawanishi et al., Voting-based Hand-Waving Gesture Spotting from a Low-Resolution Far-Infrared Image Sequence, VCIP2018.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#objpose" class="image left"><img src="https://yasutomo57jp.github.io/images/objpose.png" alt="" /></a>
              <div class="inner">
                <h3>Object Pose Estimation from Depth Images</h3>
                <h4>Summary</h4>
                <p>When a robot grasps or manipulates an object, it needs to recognize the object's pose correctly. Recently, depth image sensors have been installed in many robots. Therefore, we are working on object pose estimation using depth images.</p>
                <h4>Publications</h4>
                <ul>
                  <li>N. M. Z. Hashim et al., Best Next-Viewpoint Recommendation by Selecting Minimum Pose Ambiguity for Category-level Object Pose Estimation, JSPE, 2021.
                  <li>H. Tatemichi et al., Median-Shape Representation Learning for Category-Level Object Pose Estimation in Cluttered Environments, ICPR2020.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#gaze" class="image left"><img src="https://yasutomo57jp.github.io/images/gaze.png" alt="" /></a>
              <div class="inner">
                <h3>Gaze Analysis of a Crowd of People</h3>
                <h4>Summary</h4>
                <p>Knowing what a large number of people are paying attention to is useful for estimating the interests of people watching sports or attending a live music event. Therefore, from a video observing many people simultaneously, we are working on a research project to estimate the target most people in a video are paying attention to.</p>
                <h4>Publications</h4>
                <ul>
                  <li>Y. Kodama et al., Localizing the Gaze Target of a Crowd of People, ACCV2018 Workshop.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#posesequence" class="image left"><img src="https://yasutomo57jp.github.io/images/posesequence.png" alt="" /></a>
              <div class="inner">
                <h3>Action Recognition from a Human Skeleton Sequence</h3>
                <h4>Summary</h4>
                <p>By focusing on how a person's skeleton changes, we can estimate what the person is doing and what state the person is in. Recently, technologies that can accurately estimate a person's skeleton from images have become available, and we are working on applying the estimated skeleton to various recognition tasks.</p>
                <h4>Publications</h4>
                <ul>
                  <li>N. Nishida et al., SOANets: Encoder-Decoder based Skeleton Orientation Alignment Network for White Cane User Recognition from 2D Human Skeleton Sequence, VISAPP2020.</li>
                  <li>O. Temuroglu et al, Occlusion-Aware Skeleton Trajectory Representation for Abnormal Behavior Detection, IW-FCV2020.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#mtmct" class="image left"><img src="https://yasutomo57jp.github.io/images/mtmct.png" alt="" /></a>
              <div class="inner">
                <h3>People Tracking</h3>
                <h4>Summary</h4>
                <p>In techniques such as people flow analysis and lost child search, it is important to know the trajectories of people walking around a large area. We are working on tracking and re-identifying people within/across camera views in order to know which person went where by passing which paths, in a situation where a large area is being observed by multiple cameras.</p>
                <h4>Publications</h4>
                <ul>
                  <li>Y. Kawanishi et al., Trajectory Ensemble: Multiple Persons Consensus Tracking across Non-overlapping Multiple Cameras over Randomly Dropped Camera Networks, CVPR2017 Workshop.</li>
                </ul>
              </div>
            </div>

            <div class="box post">
              <a href="#otherfields" class="image left"><img src="https://yasutomo57jp.github.io/images/otherfields.png" alt="" /></a>
              <div class="inner">
                <h3>Image Recognition Applications to Other Research Fields</h3>
                <h4>Summary</h4>
                <p>Image recognition techniques are expected to be used in various fields such as life science. We are also working on the application of image recognition technology in the fields of astronomy and archaeology. (Detection of star forming region in Astronomy field, Automatic origin identification of earthenware in Archaeology field.)</p>
                <h4>Publications</h4>
                <ul>
                  <li>S. Ueda et al., Identification of infrared-ring structures by convolutional neural network, SPIE Astronomical Telescopes + Instrumentation 2020</li>
                </ul>
              </div>
            </div>

          </div>
        </div>
      </section>

      <!-- Footer -->
      <div id="footer">

        <!-- Icons -->
        <ul class="icons">
          <li><a href="https://twitter.com/yasutomo57jp" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
          <li><a href="https://www.facebook.com/yasutomo57jp" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
          <li><a href="https://github.com/yasutomo57jp" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
          <li><a href="#" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
          <li><a href="#" class="icon brands fa-google-plus-g"><span class="label">Google+</span></a></li>
        </ul>

        <!-- Copyright -->
        <div class="copyright">
          <ul class="menu">
            <li>&copy; Yasutomo KAWANISHI. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
          </ul>
        </div>

      </div>

    </div>

    <!-- Scripts -->
    <script src="https://yasutomo57jp.github.io/assets/js/jquery.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/jquery.dropotron.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/browser.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/breakpoints.min.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/util.js"></script>
    <script src="https://yasutomo57jp.github.io/assets/js/main.js"></script>

  </body>
</html>
